{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FrSwHWxn6ZX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un6-Sisr0lle"
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "! pip install evaluate\n",
        "!pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOVSaVJcpYAr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egrAw_hkpVUt"
      },
      "outputs": [],
      "source": [
        "%cd '/content/drive/MyDrive/Thesis'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PqkvSBIuaA-"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkj9DsadpXcT"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "start = time.time()\n",
        "print('Loading dataframes...')\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_X_train.csv\")\n",
        "X_val = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_X_val.csv\")\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_X_test.csv\")\n",
        "\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_y_train.csv\")\n",
        "y_val = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_y_val.csv\")\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/Thesis/data/HR_y_test.csv\")\n",
        "\n",
        "\n",
        "print(f'Done loading dataframe in {time.time() - start} seconds.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICVJim5sA6IT"
      },
      "outputs": [],
      "source": [
        "#Check data\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARWn5Q1ZBi1L"
      },
      "outputs": [],
      "source": [
        "#Check split on date\n",
        "print(X_train[\"date_decision\"].max())\n",
        "print(X_val[\"date_decision\"].max())\n",
        "print(X_test[\"date_decision\"].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szD78lYhrPCV"
      },
      "outputs": [],
      "source": [
        "#Check columns\n",
        "list(X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhIYqg66hZmI"
      },
      "outputs": [],
      "source": [
        "# Keep text\n",
        "X_train_text = X_train['full_text']\n",
        "X_val_text =  X_val['full_text']\n",
        "X_test_text = X_test['full_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEMmecA8kInj"
      },
      "outputs": [],
      "source": [
        "#Concatenate train and test for converging to HuggingFace Dataset\n",
        "Xy_train = pd.concat([X_train_text, y_train], axis=1)\n",
        "Xy_val = pd.concat([X_val_text, y_val], axis=1)\n",
        "Xy_test = pd.concat([X_test_text, y_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjNDAKkRMGK8"
      },
      "outputs": [],
      "source": [
        "print(Xy_train.shape)\n",
        "print(Xy_val.shape)\n",
        "print(Xy_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXjYzJtth2VV"
      },
      "outputs": [],
      "source": [
        "#Create Hugging Face Dataset\n",
        "train = Dataset.from_pandas(Xy_train, split= \"train\",preserve_index=False)\n",
        "val = Dataset.from_pandas(Xy_val, split= \"val\",preserve_index=False)\n",
        "test = Dataset.from_pandas(Xy_test, split= \"test\",preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm4DN_5oorhj"
      },
      "outputs": [],
      "source": [
        "raw_data = DatasetDict({\"train\":train , \"val\":val, \"test\":test})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh2yj7b8rNgp"
      },
      "outputs": [],
      "source": [
        "raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhVSnty_8n_e"
      },
      "outputs": [],
      "source": [
        "len(raw_data['train']['full_text'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-g0_G2IDWSZ"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw788cezue6W"
      },
      "outputs": [],
      "source": [
        "#Initialize tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "checkpoint = \"DTAI-KULeuven/robbert-2023-dutch-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get head truncated dataset\n",
        "def tokenize_function(example):\n",
        "\n",
        "    return tokenizer(example['full_text'], padding=True, truncation=True)\n",
        "\n",
        "tokenized_dataset_head_truncation = raw_data.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "zSmdyhdK1wrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_head_truncation"
      ],
      "metadata": {
        "id": "0fV29qBL2qbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_head_truncation.save_to_disk(\"HR_tokenized_dataset_head_truncated.hf\")"
      ],
      "metadata": {
        "id": "Lts7mnZn16zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iR4ihc5akYq"
      },
      "outputs": [],
      "source": [
        "# Get not truncated dataset\n",
        "def tokenize_function(example):\n",
        "\n",
        "    return tokenizer(example['full_text'], padding=False, truncation=False)\n",
        "\n",
        "tokenized_dataset_no_truncation = raw_data.map(tokenize_function, batched=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_no_truncation.save_to_disk(\"HR_tokenized_dataset_no_truncation.hf\")"
      ],
      "metadata": {
        "id": "ZiPGAFDCBzAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = tokenized_dataset_no_truncation[\"train\"][\"input_ids\"]\n",
        "n_tokens_list = [len(x) for x in examples]\n",
        "larger_512_list = [x>512 for x in n_tokens_list]"
      ],
      "metadata": {
        "id": "kfcEQFUMAEYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proportion of cases larger than 512 tokens\n",
        "sum(larger_512_list)/len(larger_512_list)"
      ],
      "metadata": {
        "id": "1im84E1-Dxzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min and max amount of tokens\n",
        "print(min(n_tokens_list))\n",
        "print(max(n_tokens_list))"
      ],
      "metadata": {
        "id": "P7uCTKR2Coxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUkiZZnwZYT1"
      },
      "outputs": [],
      "source": [
        "# Create plot n_tokens distribution\n",
        "\n",
        "# Convert list to DataFrame\n",
        "n_tokens_df = pd.DataFrame(n_tokens_list, columns=['Values'])\n",
        "bins = [0, 512, 1024, 2048, float('inf')]  # float('inf') represents infinity for the upper bound\n",
        "labels = ['< 512','>512 < 1024','>1024 < 2048', '> 2048']\n",
        "\n",
        "n_tokens_df['Category'] = pd.cut(n_tokens_df['Values'], bins=bins, labels=labels)\n",
        "\n",
        "\n",
        "# Count the occurrences of each category\n",
        "category_counts = n_tokens_df['Category'].value_counts().sort_index().reset_index()\n",
        "category_counts.columns = ['Category', 'Count']  # Rename columns for clarity\n",
        "\n",
        "# Create a bar plot using Seaborn\n",
        "sns.barplot(x='Category', y='Count', data=category_counts, palette='viridis')\n",
        "plt.xlabel('Tokens')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of n tokens')\n",
        "plt.xticks(rotation=45)  # Rotate x labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOeJWk9goDZd"
      },
      "outputs": [],
      "source": [
        "# Define function for middle truncation with tokenisation for use with HuggingFace\n",
        "\n",
        "n_tokens_list2 = []\n",
        "larger_512_count2 = 0\n",
        "\n",
        "\n",
        "def truncate_text(example):\n",
        "    global n_tokens_list2, larger_512_count2\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer(example['full_text'], add_special_tokens=True)\n",
        "    n_tokens2 = len(tokens['input_ids'])\n",
        "\n",
        "    n_tokens_list2.append(n_tokens2)\n",
        "\n",
        "    if n_tokens2 > 512:\n",
        "      larger_512_count2 += 1\n",
        "\n",
        "    # If the tokenized text is longer than 512, find the minimum index of the phrases\n",
        "    if n_tokens2 > 512:\n",
        "        min_index = len(example['full_text'])  # Set a high initial value for comparison\n",
        "        start_pos = 0 # Default to using the text from beginning if none of the phrases are found\n",
        "\n",
        "        # Declare phrases\n",
        "        phrases = [\"beoordeling van het middel\", \"beoordeling van het eerste middel\", \"beoordeling van het tweede middel\", \"beoordeling van het derde middel\", \"beoordeling van de middelen\", \"beoordeling van de ontvankelijkheid\"]\n",
        "\n",
        "        # Search for each phrase and update the start position if a phrase is found earlier\n",
        "        for phrase in phrases:\n",
        "            # Find the first occurrence of the phrase using regex for exact match\n",
        "            match = re.search(re.escape(phrase), str(example['full_text']), flags=re.IGNORECASE)\n",
        "            if match and match.start() < min_index:\n",
        "                min_index = match.start()\n",
        "                start_pos = min_index\n",
        "\n",
        "        #search for 'beoordeling' if no phrase is found\n",
        "        if start_pos == 0:\n",
        "          match = re.search(re.escape(\"beoordeling\"), str(example['full_text']), flags=re.IGNORECASE)\n",
        "          if match and match.start() < min_index:\n",
        "              min_index = match.start()\n",
        "              start_pos = min_index\n",
        "\n",
        "        # If a phrase is found, use the text from its first occurrence, otherwise start from beginning\n",
        "        text = example['full_text'][start_pos:]\n",
        "        tokens = tokenizer(text, add_special_tokens=True, truncation=True)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KirTyrNnwTn"
      },
      "outputs": [],
      "source": [
        "# Create middle truncated dataset\n",
        "tokenized_dataset = raw_data.map(truncate_text, batched=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbrglMgTpJrM"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVz4X-BZnH-q"
      },
      "outputs": [],
      "source": [
        "# Rename column y to label\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"cit_in_binary\", \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INHKm8W0paAJ"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset.save_to_disk(\"HR_tokenized_dataset.hf\")"
      ],
      "metadata": {
        "id": "uN_up_bwoR07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from datasets import load_from_disk"
      ],
      "metadata": {
        "id": "QacafbsfuB6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_encoded_dataset = load_from_disk(\"/content/drive/MyDrive/Thesis/HR_tokenized_dataset.hf\")\n"
      ],
      "metadata": {
        "id": "n89tRrK3t20c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_encoded_dataset"
      ],
      "metadata": {
        "id": "h9JfuGmuuEHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}