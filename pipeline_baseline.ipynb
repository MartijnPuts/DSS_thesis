{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c21904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MultiLabelBinarizer,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d956ba4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "court = 'hr' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fad12",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    full_date = datetime.strptime(x, '%Y-%m-%d')\n",
    "    return full_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_court_citations_amount(x, year):\n",
    "\n",
    "    ecli_list = []\n",
    "    c=0\n",
    "\n",
    "    if x is not np.nan:\n",
    "        x = eval(x)        \n",
    "        for reference in x:\n",
    "            ecli_list.append(reference[\"target_ecli\"])\n",
    "        \n",
    "        for ecli in ecli_list:\n",
    "            year_ecli = int(ecli.split(':')[3])\n",
    "            \n",
    "            if year_ecli <= year:\n",
    "                c +=1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    return c\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0245e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formal_citations(x):\n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for reference in x:\n",
    "            if \"eerdereaanleg\" in reference[\"type\"]:\n",
    "                c += 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def get_court_citations(x, chosen_court, year):\n",
    "    \"\"\" Count the number of occurrences of chosen_court in x, which is a list of references in dictionary format. \"\"\"\n",
    "    #not neccesary to exclude eerdere aanleg because they are never selected, only HR etc.\n",
    "    c = 0\n",
    "    ecli_list = []\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for reference in x:\n",
    "            ecli_list.append(reference[\"target_ecli\"])\n",
    "    \n",
    "        for ecli in ecli_list:\n",
    "            court_name = ecli.split(':')[2]\n",
    "            year_ecli = int(ecli.split(':')[3])\n",
    "            \n",
    "            if court_name == chosen_court and year_ecli <= year:\n",
    "                c +=1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_citations(x, year):\n",
    "    \n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for reference in x:\n",
    "            ecli = reference[\"target_ecli\"]\n",
    "            year_ecli = int(ecli.split(':')[3])\n",
    "            \n",
    "            if \"latereaanleg\" not in reference[\"type\"] and year_ecli >= year :\n",
    "                c += 1\n",
    "        \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outgoing_legislations(x):\n",
    "    c = 0\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for legislation in x:\n",
    "            c += 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52be03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Wraps `MultiLabelBinarizer` in a form that can work with `ColumnTransformer`. Note that input X has to be a `pandas.DataFrame`. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mlbs = list()\n",
    "        self.n_columns = 0\n",
    "        self.categories_ = self.classes_ = list()\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        for i in range(X.shape[1]): # X can be of multiple columns\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(X.iloc[:,i])\n",
    "            self.mlbs.append(mlb)\n",
    "            self.classes_.append(mlb.classes_)\n",
    "            self.n_columns += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X:pd.DataFrame):\n",
    "        if self.n_columns == 0:\n",
    "            raise ValueError('Please fit the transformer first.')\n",
    "        if self.n_columns != X.shape[1]:\n",
    "            raise ValueError(f'The fit transformer deals with {self.n_columns} columns '\n",
    "                             f'while the input has {X.shape[1]}.'\n",
    "                            )\n",
    "        result = list()\n",
    "        for i in range(self.n_columns):\n",
    "            result.append(self.mlbs[i].transform(X.iloc[:,i]))\n",
    "\n",
    "        result = np.concatenate(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        return self.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384357d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def remove_uncommon(values, col_name):\n",
    "    \"\"\" Combine values into larger categories for identifying the most informative features. `keep_list` contains all values that occur in more than 1% of the rows. \"\"\"\n",
    "\n",
    "    VALUE_MAP = {\n",
    "        \"Cassatie in het belang der wet\": \"Cassatie\",\n",
    "        \"Voorlopige voorziening+bodemzaak\": \"Voorlopige voorziening\",\n",
    "    }\n",
    "    values = [values]\n",
    "    x = [v.strip() for v in values]\n",
    "    new = \"\"\n",
    "    \n",
    "    if col_name == 'procedure':\n",
    "        global court\n",
    "        if court == 'hr':\n",
    "            keep_list = ['Cassatie', 'Cassatie in het belang der wet', 'Artikel 81 RO-zaken']\n",
    "        elif court == 'rb':\n",
    "            keep_list = ['Eerste aanleg - enkelvoudig', 'Eerste aanleg - meervoudig']\n",
    "        elif court == 'rvs':\n",
    "            keep_list = ['Hoger beroep', 'Eerste aanleg - meervoudig', 'Eerste aanleg - enkelvoudig', 'Voorlopige voorziening']\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown court\")\n",
    "            \n",
    "        for p in x:\n",
    "            if (p_converted := VALUE_MAP.get(p, p)) in keep_list:\n",
    "                new += (p_converted)\n",
    "            elif p == '-':\n",
    "                pass\n",
    "            else:\n",
    "                new += ('Other procedure')\n",
    "                \n",
    "    elif col_name == 'subject':\n",
    "        for r in x:\n",
    "            r = r.strip()\n",
    "            \n",
    "            # Only include the main part of the law area\n",
    "            if r != 'Internationaal publiekrecht' or r != '-':\n",
    "                new += (r.split(';')[0])\n",
    "            else:\n",
    "                new += 'other'\n",
    "\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a22ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(string):\n",
    "    cleaned_string = re.sub(r'\\n+', ' ', string)\n",
    "    cleaned_string_list = cleaned_string.split()\n",
    "    length = len(cleaned_string_list)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_citation_dif(x, year):\n",
    "    \n",
    "    ecli_years = []\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for reference in x:\n",
    "            ecli = reference[\"target_ecli\"]\n",
    "            year_ecli = int(ecli.split(':')[3])\n",
    "            if \"latereaanleg\" not in reference[\"type\"] and year_ecli >= year :\n",
    "                ecli_years.append(year_ecli)\n",
    "\n",
    "        if len(ecli_years) > 0:\n",
    "            first_citation = min(ecli_years)   \n",
    "            first_citation_dif = first_citation - year\n",
    "            \n",
    "            return first_citation_dif\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        return np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "start = time.time()\n",
    "print('Loading dataframe...')\n",
    "df = pd.read_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_rechtspraak_metadata_citations_full.csv\")\n",
    "print(f'Done loading dataframe in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff532158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dates to datetime\n",
    "df.loc[:,'date_decision'] = df.loc[:,'date_decision'].apply(get_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06427315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add year column\n",
    "df.loc[:,'year'] = df.loc[:,'date_decision'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9495349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add first_citation_dif_column\n",
    "df.loc[:,'first_cit_dif'] = df.apply(lambda x: get_first_citation_dif(x['citations_incoming'], x['year']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04413d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot first citation distribution\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df_graph = pd.DataFrame()\n",
    "df_graph['first_cit_dif'] = df.loc[:,'first_cit_dif']\n",
    "bins = [0, 1, 2, 3, 5, 7, 9 ,float('inf')]  # float('inf') represents infinity for the upper bound\n",
    "#labels = ['[0, 512]','[513, 1024]','[1024, 2048]', '[2048, 64784]']\n",
    "\n",
    "df_graph['Category'] = pd.cut(df_graph['first_cit_dif'], bins=bins)#, labels=labels)\n",
    "\n",
    "\n",
    "# Count the occurrences of each category\n",
    "category_counts = df_graph['Category'].value_counts().sort_index()\n",
    "category_counts = category_counts.cumsum()\n",
    "category_counts = category_counts.reset_index()\n",
    "category_counts.columns = ['Category', 'Count']  # Rename columns for clarity\n",
    "\n",
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Category', y='Count', data=category_counts, color='#0a6ca8')\n",
    "\n",
    "for i, v in enumerate(category_counts['Count']):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel('Years since ruling date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of first incoming citation for Supreme Court rulings')\n",
    "plt.gca().xaxis.set_ticklabels(['1', '2', '3', '4 - 5', '6 - 7', '8 - 9', '10 - inf'])  # Set custom labels\n",
    "\n",
    "#plt.xticks(rotation=30)  # Rotate x labels for better visibility\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(\"./figs/Distribution_tokens_HR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort dataframe by date\n",
    "df = df.sort_values(by=['date_decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by date and remove files that haven't existed for 5 years\n",
    "df = df.loc[(df['date_decision'] < datetime(2019, 4, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ce1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect values of subject\n",
    "df.loc[:,'subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert subject to less categories\n",
    "df.loc[:,'subject'] = df.loc[:,'subject'].replace(np.nan, '-')\n",
    "df.loc[:,'subject'] = df.loc[:,'subject'].map(lambda x: remove_uncommon(x, 'subject'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 1 missing value\n",
    "df.drop(df[df['subject'] == '-'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d76fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove 1 wrong value\n",
    "df.drop(df[df['subject'] == 'Internationaal publiekrecht'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'procedure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3bf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the least frequent columns together\n",
    "df.loc[:,'procedure'] = df.loc[:,'procedure'].replace(np.nan, '-')\n",
    "df.loc[:,'procedure'] = df.loc[:,'procedure'].map(lambda x: remove_uncommon(x, 'procedure'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'procedure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#incoming citations\n",
    "df.loc[:,'cit_in_count'] = df.apply(lambda x: get_incoming_citations(x['citations_incoming'], x['year']), axis = 1)\n",
    "df.loc[:,'cit_in_binary'] = np.where(df.loc[:,'cit_in_count'] > 0,1,0)\n",
    "\n",
    "# outgoing citations Create columns with counts for specific courts\n",
    "df.loc[:,'cit_out_count'] = df.apply(lambda x: get_court_citations_amount(x['citations_outgoing'], x['year']), axis=1)\n",
    "\n",
    "df.loc[:,'cit_out_sc_count'] = df.apply(lambda x: get_court_citations(x['citations_outgoing'],'HR', x['year']), axis=1)\n",
    "df.loc[:,'cit_out_cs_count'] = df.apply(lambda x: get_court_citations(x['citations_outgoing'],'RVS', x['year']), axis=1)\n",
    "df.loc[:,'cit_out_cbb_count'] = df.apply(lambda x: get_court_citations(x['citations_outgoing'],'CBB', x['year']), axis=1)\n",
    "df.loc[:,'cit_out_crvb_count'] = df.apply(lambda x: get_court_citations(x['citations_outgoing'],'CRVB', x['year']), axis=1)\n",
    "\n",
    "\n",
    "df.loc[:,'cit_out_supremes_count'] = df.loc[:,'cit_out_cbb_count'] + df.loc[:,'cit_out_crvb_count'] + df.loc[:,'cit_out_cs_count'] + df.loc[:,'cit_out_sc_count']\n",
    "df.loc[:,'cit_out_not_supremes_count'] = df.loc[:,'cit_out_count'] - df.loc[:,'cit_out_supremes_count']\n",
    "\n",
    "#formal relations\n",
    "df.loc[:,'cit_phr_count'] = df.apply(lambda x: get_court_citations(x['citations_outgoing'],'PHR', x['year']), axis=1)\n",
    "\n",
    "df.loc[:,'cit_formal_count'] = df.loc[:,'citations_outgoing'].apply(lambda x: get_formal_citations(x))\n",
    "df.loc[:,'cit_not_phr_count'] = df.loc[:,'cit_formal_count'] - df.loc[:,'cit_phr_count']\n",
    "\n",
    "# Law count \n",
    "df.loc[:,'legislation_count'] = df['legislations_cited'].apply(lambda x: get_outgoing_legislations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f42ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'cit_in_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'legislation_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a656d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove repeated newline characters\n",
    "#Summary\n",
    "df.loc[:,'summary'] = df.loc[:,'summary'].apply(lambda x: re.sub(r'(\\n\\s*)+', ' \\n', x))\n",
    "\n",
    "# decicion\n",
    "df.loc[:,'full_text'] = df.loc[:,'full_text'].apply(lambda x: re.sub(r'(\\n\\s*)+', ' \\n', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lenghts\n",
    "#summary\n",
    "df.loc[:,'summary_length'] = df.loc[:,'summary'].apply(lambda x: get_length(x))\n",
    "\n",
    "# decicion\n",
    "df.loc[:,'full_text_length'] = df.loc[:,'full_text'].apply(lambda x: get_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().applymap(lambda x: f\"{x:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc925a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_full_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5411da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('Loading dataframe...')\n",
    "df = pd.read_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_full_processed.csv\")\n",
    "print(f'Done loading dataframe in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    full_date = datetime.strptime(x, '%Y-%m-%d')\n",
    "    return full_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incoming_citations(x, year):\n",
    "    \n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    if x is not np.nan:\n",
    "        x = eval(x)\n",
    "        for reference in x:\n",
    "            ecli = reference[\"target_ecli\"]\n",
    "            year_ecli = int(ecli.split(':')[3])\n",
    "            \n",
    "            if \"latereaanleg\" not in reference[\"type\"] and year_ecli >= year :\n",
    "                c += 1\n",
    "        \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot incoming citations distribution\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df_graph = pd.DataFrame()\n",
    "df_graph['cit_in_count'] = df.loc[:,'cit_in_count']\n",
    "bins = [-0.01,0.99,2, 5, 10, 15, 20, 25, float('inf')]  # float('inf') represents infinity for the upper bound\n",
    "#labels = ['[0, 512]','[513, 1024]','[1024, 2048]', '[2048, 64784]']\n",
    "\n",
    "df_graph['Category'] = pd.cut(df_graph['cit_in_count'], bins=bins)#, labels=labels)\n",
    "\n",
    "\n",
    "# Count the occurrences of each category\n",
    "category_counts = df_graph['Category'].value_counts().sort_index()\n",
    "category_counts = category_counts.reset_index()\n",
    "category_counts.columns = ['Category', 'Count'] # Rename columns for clarity\n",
    "\n",
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Category', y='Count', data=category_counts, color='#0a6ca8')\n",
    "\n",
    "for i, v in enumerate(category_counts['Count']):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "plt.xlabel('Incoming citations')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of incoming citations for Supreme Court rulings')\n",
    "\n",
    "# Customize x-axis ticks and labels\n",
    "#plt.xticks(rotation=30)  # Positions for the ticks (excluding infinity)\n",
    "plt.gca().xaxis.set_ticklabels(['0', '1 - 2', '3 - 5', '6 - 10', '11 - 15', '16 - 20', '21 - 25', '26 - inf'])  # Set custom labels\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(\"./figs/Distribution_tokens_HR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def sample_together(n, X, y):\n",
    "    \"\"\"This function is used to keep X and Y together during undersampling\"\"\"\n",
    "\n",
    "    random.seed(0)\n",
    "    rows = random.sample(np.arange(0,len(X.index)).tolist(),n)\n",
    "\n",
    "    return X.iloc[rows,], y.iloc[rows,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def undersample(X, y, under=0):\n",
    "    \"\"\" Balance the data to the size of the smallest class. \"\"\"\n",
    "    \n",
    "    y_min = y[y == under]\n",
    "    y_max = y[y != under]\n",
    "    X_min = X.filter(y_min.index, axis=0)\n",
    "    X_max = X.filter(y_max.index, axis=0)\n",
    "\n",
    "    X_under, y_under = sample_together(len(y_min.index), X_max, y_max)\n",
    "    \n",
    "    X = pd.concat([X_under, X_min])\n",
    "    y = pd.concat([y_under, y_min])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475a37e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def split_data(data, features, dev, balance_test_set=False):\n",
    "    \"\"\" Create train, test, and dev sets. \"\"\"\n",
    "\n",
    "    print('________________________ Creating train and test data __________________________')\n",
    "\n",
    "    data = data.sort_values(by=['date_decision'])\n",
    "\n",
    "    # Get train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[features],\n",
    "        data['cit_in_binary'], test_size=0.12, shuffle=False)\n",
    "\n",
    "    if dev:\n",
    "        # Split train in dev and train\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train[features],\n",
    "        y_train, test_size=0.136, shuffle=False)\n",
    "\n",
    "    # Balance train\n",
    "    #X_train, y_train = undersample(X_train, y_train)\n",
    "\n",
    "    #if balance_test_set:\n",
    "        #X_test, y_test = undersample(X_test, y_test)\n",
    "\n",
    "    print('Training data:', X_train.shape, '\\n', y_train.value_counts(), '\\n')\n",
    "    print('Validation data:', X_val.shape, '\\n', y_val.value_counts(), '\\n')\n",
    "    print(\"Test data:\", X_test.shape, '\\n', y_test.value_counts(), '\\n')\n",
    "    print()\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a751500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "def create_pipeline(categorical_features, numerical_features, decision_features, summary_features):\n",
    "    \"\"\" Create the pipeline, including transformers for numerical, categorical, and textual features. The vectorizer, n-gram length and analyzer can be changed here. \"\"\"\n",
    "    \n",
    "    transformers = []\n",
    "        \n",
    "    categorical_transformer = MultiHotEncoder()\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[('scaler', StandardScaler())]\n",
    "    )\n",
    "    decision_transformer = TfidfVectorizer(analyzer='word', ngram_range=(3,3))\n",
    "    summary_transformer = TfidfVectorizer(analyzer='word', ngram_range=(3,3))\n",
    "    \n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
    "    if numerical_features:\n",
    "        transformers.append(('num', numeric_transformer, numerical_features))\n",
    "    if decision_features:\n",
    "        transformers.append(('full_text', decision_transformer, 'full_text'))\n",
    "    if summary_features:\n",
    "        transformers.append(('summary', summary_transformer, 'summary'))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop',\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor), \n",
    "            (\"debug\", Debug()),\n",
    "            ('classifier', LinearSVC(dual= True, random_state=0, max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3409e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['creator', 'zaaknummer', 'issued', 'inhoudsindicatie', \"hasVersion\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91daf1fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "# Select features and create list to extract from dataframe. Uncomment a feature to use it.\n",
    "\n",
    "features_num = [\n",
    "         'full_text_length', \n",
    "         'summary_length',\n",
    "         'legislation_count',\n",
    "         'cit_out_not_supremes_count',\n",
    "         'cit_out_supremes_count',\n",
    "         'cit_not_phr_count',\n",
    "         'cit_phr_count',\n",
    "          'cit_formal_count',  \n",
    "]\n",
    "\n",
    "features_cat = [\n",
    "         'procedure', \n",
    "         'subject', \n",
    "]\n",
    "\n",
    "# put 'True' to use a feature in the model\n",
    "features_decision = True\n",
    "features_summary = True\n",
    "\n",
    "feature_list = features_num + features_cat \n",
    "\n",
    "feature_list.append('date_decision')\n",
    "feature_list.append('ecli')\n",
    "\n",
    "if features_decision:\n",
    "    feature_list.append('full_text')\n",
    "if features_summary:\n",
    "    feature_list.append('summary')\n",
    "\n",
    "print('Selected features:', feature_list)\n",
    "print()\n",
    "start = time.time()\n",
    "\n",
    "# Create train and test data, indicate if you want to use development data\n",
    "\n",
    "create_dev_data = True\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(df, feature_list, create_dev_data)\n",
    "\n",
    "\n",
    "print('Time passed:', time.time() - start)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[\"date_decision\"].max())\n",
    "print(X_val[\"date_decision\"].max())\n",
    "print(X_test[\"date_decision\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_X_train.csv\", index=False)\n",
    "X_val.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_X_val.csv\", index=False)\n",
    "X_test.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_X_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_y_train.csv\", index=False)\n",
    "y_val.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_y_val.csv\", index=False)\n",
    "y_test.to_csv(\"D:\\DSS D-schijf\\Thesis\\data\\HR_y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1db93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:\\DSS D-schijf\\Thesis\\data\\HR_X_train.npy\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPTED FROM Schepers et al. (2023)\n",
    "\n",
    "# Create pipeline\n",
    "clf = create_pipeline(features_cat, features_num, features_decision, features_summary)\n",
    "\n",
    "# Start train-test phase    \n",
    "start = time.time()\n",
    "    \n",
    "print('Training...')\n",
    "print(X_train.shape)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Testing...')\n",
    "print(X_val.shape)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print('_____________________ Classification Report ___________________________')\n",
    "class_report = classification_report(y_val, y_pred, labels=[0, 1])\n",
    "print(class_report)\n",
    "print()\n",
    "print('\\n_____________________ Confusion Matrix _______________________________')\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "print(conf_matrix)\n",
    "print(\"\\n_____________________ Matthew's Correlation Coefficient ______________\")\n",
    "matt_coef = matthews_corrcoef(y_val, y_pred)\n",
    "print(matt_coef)\n",
    "timing = time.time() - start\n",
    "print('Total time passed:', timing)\n",
    "\n",
    "# Write to output file\n",
    "\n",
    "out_str = 'Features used:' + str(feature_list) + '\\n'\n",
    "\n",
    "with open(\"D:\\DSS D-schijf\\Thesis\\HR_baseline.txt\", 'a') as out:\n",
    "    out.write(court)\n",
    "    out.write('\\n')\n",
    "    out.write(out_str)\n",
    "    out.write('Development Data:')\n",
    "    out.write(str(create_dev_data))\n",
    "    out.write('\\n')\n",
    "    out.write('X_train:')\n",
    "    out.write(str(X_train.shape))\n",
    "    out.write('\\n')\n",
    "    out.write(str(y_train.value_counts()))\n",
    "    out.write('\\n')\n",
    "    out.write('X_test:')\n",
    "    out.write(str(X_test.shape))\n",
    "    out.write('\\n')\n",
    "    out.write(str(y_test.value_counts()))\n",
    "    out.write('\\n')\n",
    "    out.write(str(class_report))\n",
    "    out.write('\\n')\n",
    "    out.write(str(conf_matrix))\n",
    "    out.write('\\n')\n",
    "    out.write('Matt coeff: ' + str(matt_coef))\n",
    "    out.write('\\n')\n",
    "    out.write('total time:' + str(timing))\n",
    "    out.write('\\n_______________________________________________________')\n",
    "    out.write('\\n')\n",
    "    \n",
    "\n",
    "    out.write('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
