{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HXGxt64OYXA7"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdKId6IXLRf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Thesis'"
      ],
      "metadata": {
        "id": "oIiutU8Mw5jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "!pip3 install datasets"
      ],
      "metadata": {
        "id": "xwN8U0dtXXWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset, load_from_disk, concatenate_datasets, DatasetDict\n",
        "\n"
      ],
      "metadata": {
        "id": "H5qETrrHXZ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define checkpoint and initialize tokenizer\n",
        "checkpoint= \"DTAI-KULeuven/robbert-2023-dutch-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ],
      "metadata": {
        "id": "bwtGF39F9i-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "HXGxt64OYXA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import dataset\n",
        "dataset = load_dataset('multi_eurlex', 'nl')"
      ],
      "metadata": {
        "id": "l7n20kc3YT2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check dataset\n",
        "dataset"
      ],
      "metadata": {
        "id": "sRbXQtrxZ4Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check text in dataset\n",
        "dataset[\"train\"][\"text\"][2]"
      ],
      "metadata": {
        "id": "86TUBR25d5_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove unneeded columns\n",
        "dataset = dataset.remove_columns([\"labels\", \"celex_id\"])\n"
      ],
      "metadata": {
        "id": "O1RLKkk-hNT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "3uxUHbSRhW5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define tokenizer\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "#tokenize data\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "B3QlOPmjqBZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "dq1Vyn01qNBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create chunked data\n",
        "\n",
        "chunk_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "Y2kpEEFiqiPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_dataset"
      ],
      "metadata": {
        "id": "Itu3SvIYs2YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset.save_to_disk(\"EURLEX_tokenized_dataset_full.hf\")"
      ],
      "metadata": {
        "id": "std-RhXT1swJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset.save_to_disk(\"/content/drive/MyDrive/thesis/EURLEX_tokenized_dataset_input_ids.hf\")"
      ],
      "metadata": {
        "id": "5Y_ysbXbo1Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "4_G0hrO-YUrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_encoded_dataset = load_from_disk(\"/content/drive/MyDrive/Thesis/EURLEX_tokenized_dataset_input_ids_collapsed.hf\")\n"
      ],
      "metadata": {
        "id": "ndupgK974Rh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded_encoded_dataset"
      ],
      "metadata": {
        "id": "iF2h2dM7IhE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#collapse train and validation\n",
        "reloaded_encoded_dataset[\"train\"] = concatenate_datasets([reloaded_encoded_dataset[\"train\"], reloaded_encoded_dataset[\"validation\"]])\n",
        "data = DatasetDict({key: dataset for key, dataset in reloaded_encoded_dataset.items() if key != \"validation\"})\n"
      ],
      "metadata": {
        "id": "pYTBmrgs4WkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "c_tk1QyAJw7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.save_to_disk(\"/content/drive/MyDrive/Thesis/EURLEX_tokenized_dataset_input_ids_collapsed.hf\")"
      ],
      "metadata": {
        "id": "8fmGg80DKbk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "5BhjKFPt-S8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "SHdnLNNyvlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "mPBudnbcXqbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training arguments\n",
        "batch_size = 64\n",
        "logging_steps = 5000\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "                                  output_dir = 'RobBERT-legal',\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  num_train_epochs=1,\n",
        "\n",
        "                                  learning_rate=1e-5,\n",
        "                                  lr_scheduler_type=\"polynomial\",\n",
        "                                  warmup_ratio=0.1,\n",
        "                                  weight_decay=0.1,\n",
        "\n",
        "\n",
        "                                  optim='adamw_torch',\n",
        "\n",
        "                                  push_to_hub=True,\n",
        "                                  fp16=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  remove_unused_columns=True,\n",
        "\n",
        "\n",
        "                                  )"
      ],
      "metadata": {
        "id": "EEs07xwsc8vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "e2fh0Ar9dDL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get perplexity before further pre-training\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "x5QXv6qEymat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "b-FPrRv6lc46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get perplexity after further pre-training\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "eZSaw5bTynbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}